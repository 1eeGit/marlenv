{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notebook for initial model training\n",
    "\n",
    "obs layers: \n",
    "\n",
    "    o: borders, \n",
    "\n",
    "    1: apples, \n",
    "\n",
    "    2: heads_other, \n",
    "\n",
    "    3: bodies_other, \n",
    "\n",
    "    4: tails_other,\n",
    "\n",
    "    5: head_self,\n",
    "\n",
    "    6: body_self,\n",
    "\n",
    "    7: tail_self\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/1eeGit/marlenv.git\n",
    "# !cd marlenv\n",
    "# !git checkout 1eeGit-patch-1\n",
    "# !pip install -e .\n",
    "\n",
    "### install additional packages\n",
    "# !pip install pygame==2.6.0\n",
    "# !pip install matplotlib\n",
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.12.5, pytest-8.3.2, pluggy-1.5.0\n",
      "rootdir: /home/unix\n",
      "plugins: anyio-4.4.0\n",
      "collected 5 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "marlenv/tests/test_snake.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                        [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 0.74s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test game window\n",
    "# !python video_recorder_and_input_handling.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### reload the project if needed\n",
    "# import importlib\n",
    "# import marlenv\n",
    "# importlib.reload(marlenv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import gym\n",
    "import marlenv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "## check gpu availability\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_reward_dict = {\n",
    "    'fruit': 1.0,\n",
    "    'kill': 1.5,\n",
    "    'lose': -10.0,\n",
    "    'time': 0.1,\n",
    "    'win': 10.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create the environment\n",
    "\n",
    "env = gym.make(\n",
    "    'Snake-v1',\n",
    "    height=20,       # Height of the grid map\n",
    "    width=20,        # Width of the grid map\n",
    "    num_snakes=4,    # Number of snakes to spawn on grid\n",
    "    snake_length=3,  # Initial length of the snake at spawn time\n",
    "    vision_range=5,  # Vision range (both width height), map returned if None\n",
    "    frame_stack=1,   # Number of observations to stack on return\n",
    "    reward_func=custom_reward_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNQNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(CNNQNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_shape[0] * input_shape[-1], out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        conv_out_size = self._get_conv_output((input_shape[0] * input_shape[-1], *input_shape[1:3]))  # Adjust for new input size\n",
    "        \n",
    "        self.fc1 = nn.Linear(conv_out_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        o = torch.zeros(1, *shape)\n",
    "        o = self.conv1(o)\n",
    "        o = self.conv2(o)\n",
    "        o = self.conv3(o)\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x, device=\"cpu\"):\n",
    "        x = torch.Tensor(x).to(device)\n",
    "        # Flatten the last two dimensions into the channel dimension\n",
    "        x = x.view(x.size(0), -1, *x.size()[2:])\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(env, q_network, replay_buffer, optimizer, num_episodes=500, batch_size=64, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    q_network.to(device)\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        print(\"Original state shape:\", state.shape)\n",
    "\n",
    "        # Reshape the state to combine the first and last dimensions into the channels dimension\n",
    "        state = state.reshape(state.shape[0] * state.shape[3], state.shape[1], state.shape[2])\n",
    "\n",
    "        # Add the batch dimension\n",
    "        state = state.reshape(1, *state.shape)\n",
    "\n",
    "        # Print the new shape\n",
    "        print(\"New state shape:\", state.shape)\n",
    "\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            # Epsilon-greedy action selection\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    action = q_network(state).argmax().item()\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Reshape next_state for CNN input\n",
    "            next_state = next_state.transpose(2, 0, 1)\n",
    "            next_state = next_state.reshape(1, *next_state.shape)\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            # Store transition in replay buffer\n",
    "            replay_buffer.put((state, action, reward, next_state, done))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        # Training logic here (sampling from replay buffer, etc.)\n",
    "\n",
    "\n",
    "            # Training the Q-Network\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                s_batch, a_batch, r_batch, s_prime_batch, done_batch = replay_buffer.sample(batch_size)\n",
    "\n",
    "                s_batch = torch.FloatTensor(s_batch).to(device)\n",
    "                a_batch = torch.LongTensor(a_batch).to(device)\n",
    "                r_batch = torch.FloatTensor(r_batch).to(device)\n",
    "                s_prime_batch = torch.FloatTensor(s_prime_batch).to(device)\n",
    "                done_batch = torch.FloatTensor(done_batch).to(device)\n",
    "\n",
    "                q_values = q_network(s_batch, device).gather(1, a_batch.unsqueeze(1)).squeeze(1)\n",
    "                next_q_values = q_network(s_prime_batch, device).max(1)[0]\n",
    "                target_q_values = r_batch + gamma * next_q_values * (1 - done_batch)\n",
    "\n",
    "                loss = F.mse_loss(q_values, target_q_values)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        epsilon = max(epsilon * epsilon_decay, epsilon_end)\n",
    "        print(f\"Episode {episode + 1}: Total Reward: {total_reward}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_limit):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "\n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "\n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append(a)\n",
    "            r_lst.append(r)\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append(done_mask)\n",
    "\n",
    "        return np.array(s_lst), np.array(a_lst), \\\n",
    "            np.array(r_lst), np.array(s_prime_lst), \\\n",
    "            np.array(done_mask_lst)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original state shape: (4, 11, 11, 8)\n",
      "New state shape: (1, 32, 11, 11)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m replay_buffer \u001b[38;5;241m=\u001b[39m ReplayBuffer(buffer_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n\u001b[1;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(q_network\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrain_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[45], line 30\u001b[0m, in \u001b[0;36mtrain_dqn\u001b[0;34m(env, q_network, replay_buffer, optimizer, num_episodes, batch_size, gamma, epsilon_start, epsilon_end, epsilon_decay)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     28\u001b[0m         action \u001b[38;5;241m=\u001b[39m q_network(state)\u001b[38;5;241m.\u001b[39margmax()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 30\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Reshape next_state for CNN input\u001b[39;00m\n\u001b[1;32m     33\u001b[0m next_state \u001b[38;5;241m=\u001b[39m next_state\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/RL/lib/python3.12/site-packages/gym/wrappers/order_enforcing.py:13\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 13\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observation, reward, done, info\n",
      "File \u001b[0;32m~/marlenv/marlenv/envs/snake_env.py:199\u001b[0m, in \u001b[0;36mSnakeEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(actions, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    198\u001b[0m     actions \u001b[38;5;241m=\u001b[39m [actions]\n\u001b[0;32m--> 199\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(actions) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_snakes\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, ac \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(actions):\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ac, np\u001b[38;5;241m.\u001b[39mndarray):\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_shape = env.observation_space.shape  # Should be (channels, width, height)\n",
    "num_actions = env.action_space.n  # Number of possible actions\n",
    "\n",
    "q_network = CNNQNetwork(input_shape, num_actions)\n",
    "replay_buffer = ReplayBuffer(buffer_limit=10000)\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=0.001)\n",
    "\n",
    "train_dqn(env, q_network, replay_buffer, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = q_network(state, device=\"cuda\").argmax().item()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    env.render()  # Assuming you want to see the game\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
